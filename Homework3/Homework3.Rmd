---
title: "Homework3"
author: "LeTicia Cancel, Chi Pong, Euclid Zhang, Jie Zou, Joseph Connolly"
date: "3/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE, error=FALSE}
#libraries
library("patchwork")
library("faraway")
library("pROC")
library(corrplot)
library(reshape2)
library(ggplot2)
library(dplyr)
library(psych)
library(MASS)
library(car)
library(corrplot)
library(caret)
library(performance) # overall model performace
library(AICcmodavg) # model selection
library(bestglm)
source("http://www.sthda.com/upload/rquery_cormat.r")
```

# Data Exploration

```{r}
train_df <- read.csv("https://raw.githubusercontent.com/ezaccountz/DATA_621/main/HW3/crime-training-data_modified.csv")
test_df <- read.csv("https://raw.githubusercontent.com/ezaccountz/DATA_621/main/HW3/crime-evaluation-data_modified.csv")
```

From the summary, it does not seem to have missing value. However, variables like 'zn', 'indus', 'age' and so on are skewed, additionally, variable 'chas' and response variable should be factor type instead of int.
```{r}
summary(train_df)
```

### plots

from boxplot, predictors are split by response variable, except 'nox', 'rad', and dummy variable 'chas' have no outliers, the rest of them have more/less outliers

```{r fig.height=10, fig.width=10}
par(mfrow=c(4,3))
predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target")]

for(preditor in predictors) {
  boxplot(train_df[,preditor]~train_df$target,main=preditor, 
          xlab = "target", ylab = preditor)
}
```


As I mentioned above, there are some variables skewed(not include dummy variable), some are bi-model, only 'rm' looks pretty normal.

```{r fig.height=6, fig.width=10}
# check normality and skewness
train_df.m <- melt(train_df, id.vars = 'target')%>% mutate(target = as.factor(target))
train_df.m %>% ggplot(aes(x = value)) + geom_density() + facet_wrap(~variable, scale = 'free') + theme_classic()
```


## * **Correlations**

Now let's look at the correlations between the variables  
```{r fig.height=10, fig.width=10}
corrplot(cor(train_df, use = "na.or.complete"), method = 'number', type = 'lower', diag = FALSE, tl.srt = 0.1)
```



## Data Transformation

### transform type

```{r}
train_df$chas <- as.factor(data$chas)
train_df$target <- as.factor(data$target)
```

### transform variable

corresponding library is required to perform the function, [reference is here](https://machinelearningmastery.com/pre-process-your-dataset-in-r/).

use Box-Cox transformation as well as center and scale, just so variables are in the same scale, from the plot, we see that variables are more normal than raw data. Even though there are some variables still look problematic, let's stay with it now.

```{r warning=FALSE}
# use transform data using boxcox, in the mean time center and scale them.
preprocess <- preProcess(train_df %>% dplyr::select(-target), method = c('BoxCox', 'center', 'scale'))

# get transformed data set=
trans.df <- predict(preprocess, data %>% dplyr::select(-target)) %>% mutate(target = as.factor(data$target))

# plot density to check skewness and normality
trans.df.m <- trans.df %>% mutate(target = as.factor(data$target)) %>% melt(id.vars = 'target')

trans.df.m %>% ggplot(aes(x = value)) + geom_density() + facet_wrap(~variable, scale = 'free')
```


According to the text book *A Modern Approach To Regression With R*, "when the predictor variable X has a Poisson distribution, the log odds are a linear function of x". Let's check if any of the predictors follows a Poisson distribution

```{r}
#Method of possion distribution test is from https://stackoverflow.com/questions/59809960/how-do-i-know-if-my-data-fit-a-poisson-distribution-using-r

#two tail test
p_poisson <- function(x) {
  return (1-2 * abs((1 - pchisq((sum((x - mean(x))^2)/mean(x)), length(x) - 1))-0.5))
}

predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target","chas","zn_y")]

data.frame(mean = round(apply(train_df[,predictors],2,mean),2), 
           variance = round(apply(train_df[,predictors],2,var),2),
           probability_of_poisson = round(apply(train_df[,predictors],2,p_poisson),2))
```


None of the predictors follows a poisson distribution

```{r fig.height=6, fig.width=10}
target_factored <- as.factor(train_df$target)

plot_zn <- ggplot(train_df, aes(x=zn, color=target_factored)) + geom_density()
plot_indus <- ggplot(train_df, aes(x=indus, color=target_factored)) + geom_density()
plot_nox <- ggplot(train_df, aes(x=nox, color=target_factored)) + geom_density()
plot_rm <- ggplot(train_df, aes(x=rm, color=target_factored)) + geom_density()
plot_age <- ggplot(train_df, aes(x=age, color=target_factored)) + geom_density()
plot_dis <- ggplot(train_df, aes(x=dis, color=target_factored)) + geom_density()
plot_rad <- ggplot(train_df, aes(x=rad, color=target_factored)) + geom_density()
plot_tax <- ggplot(train_df, aes(x=tax, color=target_factored)) + geom_density()
plot_prtatio <- ggplot(train_df, aes(x=ptratio, color=target_factored)) + geom_density()
plot_lstat <- ggplot(train_df, aes(x=lstat, color=target_factored)) + geom_density()
plots_medv <- ggplot(train_df, aes(x=medv, color=target_factored)) + geom_density()

plot_zn+plot_indus+plot_nox+plot_rm+plot_age+plot_dis+plot_rad+plot_tax+
  plot_prtatio+plot_lstat+plots_medv+plot_layout(ncol = 4, guides = "collect")
```


The distributions for rm with target = 0 and target = 1 are approximately normal with the same variance. Hence we don't need to transform the variable
The distributions for lstat and medv are skewed for both target = 0 and target = 1, we will add a log-transformed variable for each of them

The distributions for indus, nox, age, dis, tax, ptratio look significantly different for the target values. Let perform a anova tests on the single preditor models to see if adding a log transformed or a quadratic transformed variable will improve the performance.


```{r message=FALSE, warning=FALSE}
predictors <- c("indus", "nox", "age", "dis", "tax", "ptratio")

n <- length(predictors)

model_compare <- data.frame(
    model_1 = paste0("target~",predictors),
    model_2 = paste0("target~",predictors,"+I(",predictors,"^2)"),
    Diff_DF = rep(0,n),
    Diff_Deviance = rep(0.0000,n),
    Pr_Gt_Chi = rep(0.0000,n)
)

for (i in (1:n)) {
    test_model_1 <- glm(target~train_df[,predictors[i]],family = binomial, train_df)
    test_model_2 <- glm(target~train_df[,predictors[i]]+I(train_df[,predictors[i]]^2),family = binomial, train_df)
    anova_test <- anova(test_model_1,test_model_2,test="Chi")
    model_compare[i,3] <- anova_test$Df[2]
    model_compare[i,4] <- round(anova_test$Deviance[2],2)
    model_compare[i,5] <- round(anova_test$`Pr(>Chi)`[2],6)
}

model_compare
```


```{r message=FALSE, warning=FALSE}
predictors <- c("indus", "nox", "age", "dis", "tax", "ptratio")

n <- length(predictors)

model_compare <- data.frame(
    model_1 = paste0("target~",predictors),
    model_2 = paste0("target~",predictors,"+I(log(",predictors,"))"),
    Diff_DF = rep(0,n),
    Diff_Deviance = rep(0.0000,n),
    Pr_Gt_Chi = rep(0.0000,n)
)

for (i in (1:n)) {
    test_model_1 <- glm(target~train_df[,predictors[i]],family = binomial, train_df)
    test_model_2 <- glm(target~train_df[,predictors[i]]+I(log(train_df[,predictors[i]])),family = binomial, train_df)
    anova_test <- anova(test_model_1,test_model_2,test="Chi")
    model_compare[i,3] <- anova_test$Df[2]
    model_compare[i,4] <- round(anova_test$Deviance[2],2)
    model_compare[i,5] <- round(anova_test$`Pr(>Chi)`[2],6)
}

model_compare
```

## Building Model

For indus, the improvement is bigger by adding the squared term. For ptratio, since the distribution is left-skewed, it may be better to add the squared term. For other variables, no transformation is needed.


```{r}
train_df$log_lstat <- log(train_df$lstat)
train_df$log_medv <- log(train_df$medv)
train_df$indus_squared <- train_df$indus^2
train_df$ptratio_squared <- train_df$ptratio^2
```


```{r warning=FALSE}
predictors <- colnames(train_df)
predictors <- predictors[!predictors %in% c("target","chas","zn_y","log_lstat","log_medv","indus_squared","ptratio_squared")]

interaction_test <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(interaction_test) <- c("Preditor","Interaction","Pr_Gt_Chi")
class(interaction_test$Pr_Gt_Chi) = "Numeric"

for (predictor in predictors) {
  interaction_test[nrow(interaction_test) + 1,] <- 
    c(predictor, paste0(predictor, ":chas"), 
      round(anova(glm(target ~ train_df[,predictor]*chas,data = train_df, family = "binomial"),test="Chi")[4,5],4))
}

interaction_test
```

We will add an interaction between **tax** and **chas** and an interaction between **rad** and **chas** to our preditor candidates.
```{r}
train_df$tax_chas <- train_df$tax * train_df$chas
train_df$rad_chas <- train_df$rad * train_df$chas 
```

```{r warning=FALSE}
full_model <- glm(target~.,family = binomial, train_df)
```

```{r}
summary(full_model)
```

```{r warning=FALSE}
model_AIC <- step(full_model, trace=0)
```

```{r}
summary(model_AIC)
```


```{r warning=FALSE}
drop1(glm(target ~ .-rad_chas-zn-lstat-log_lstat, family=binomial, train_df), test="Chi")
```
```{r warning=FALSE}
model_chi <- glm(target ~ .-rad_chas-zn-lstat-log_lstat, family=binomial, train_df)
```

```{r}
summary(model_chi)
```


```{r warning=FALSE}
model_p <- glm(target~.-rad_chas-tax_chas-chas-zn-log_medv-rm,family = binomial, train_df)
summary(model_p)
```


```{r}
model_compare <- data.frame(
    model = c("full_model","model_AIC","model_chi","model_p"),
    Deviance = rep(0.0000,4),
    AIC = rep(0.0000,4),
    Accurarcy = rep(0.0000,4),
    Sensitivity = rep(0.0000,4),
    Specificity = rep(0.0000,4),
    Precision = rep(0.0000,4),
    F1 = rep(0.0000,4),
    AUC = rep(0.0000,4),
    Nagelkerke_R_squared = rep(0.0000,4)
)
```

```{r message=FALSE, warning=FALSE}
models <- list(full_model, model_AIC, model_chi, model_p)

for (i in c(1:4)) {
  predicted_class <- ifelse(models[[i]]$fitted.values>0.5,1,0)
  confusion_matrix <- confusionMatrix(as.factor(predicted_class),
                                      as.factor(train_df$target),positive = "1")
  
  model_compare[i,] <- c(model_compare[i,1],round(models[[i]]$deviance,4), models[[i]]$aic, 
                            confusion_matrix$overall[1],
                            confusion_matrix$byClass[1],
                            confusion_matrix$byClass[2],
                            confusion_matrix$byClass[3],
                            2*confusion_matrix$byClass[1]*confusion_matrix$byClass[3]/
                              (confusion_matrix$byClass[1]+confusion_matrix$byClass[3]),
                            auc(roc(train_df$target, models[[i]]$fitted.values)),
                            (1-exp((models[[i]]$dev-models[[i]]$null)/
                                     length(models[[i]]$residuals)))/
                              (1-exp(-models[[i]]$null/length(models[[i]]$residuals)))
                            )
}

```

```{r}
model_compare[,c(2:10)] <- sapply(model_compare[,c(2:10)],as.numeric)
model_compare
```




```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
marginalModelPlots(model_chi,~nox+age+dis+rad+tax+ptratio+medv,layout =c(3,3))
```


```{r}

residual_df <- mutate(train_df, residuals=residuals(model_AIC,type="deviance"), linpred=predict(model_chi,type = "link"))

gdf <- group_by(residual_df, cut(linpred, breaks=unique(quantile(linpred,(1:100)/101))))

diagdf <- summarise(gdf, residuals=mean(residuals), linpred=mean(linpred))

plot(residuals ~ linpred, diagdf, xlab="linear predictor",xlim=c(-20,20))
```



```{r fig.height=3, fig.width=3}

predictors <- c("nox","age","dis","rad","tax","ptratio","medv")

residual_df <- mutate(train_df, residuals=residuals(model_chi,type="deviance"))
gg_plots <- list()

for (i in c(1:length(predictors))) {
    gdf <- group_by(residual_df, .dots = predictors[i])
    diagdf <- summarise(gdf, residuals=mean(residuals))
    print(ggplot(diagdf, aes_string(x=predictors[i],y="residuals")) + geom_point())
}
```

```{r}
qqnorm(residuals(model_chi))
```


```{r}
halfnorm(hatvalues(model_chi))
```
```{r}
train_df[c(14,37),]
```

```{r}
predict(model_chi,train_df[c(14,37),], type="link")
```




```{r}
test_df$zn_y <- 0
test_df$zn_y[test_df$zn>0] <- 1

test_df$indus_squared <- test_df$indus^2
test_df$ptratio_squared <- test_df$ptratio^2

test_df$log_lstat <- log(test_df$lstat)
test_df$log_medv <- log(test_df$medv)

test_df$tax_chas <- test_df$tax * test_df$chas
test_df$rad_chas <- test_df$rad * test_df$chas 
```


```{r}
test_df$predicted_class <- ifelse(predict(model_chi,test_df, type = "response") >0.5,1,0)
```



```{r}
hist(test_df$predicted_class, main = "model_AIC prediction", xlab="predicted value")
```




