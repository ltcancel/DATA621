---
title: "621-hw3"
author: "Jie Zou, Euclid Zhang, Leticia Cancel, Joseph Connolly, Dennis Pong"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, message=FALSE, include=FALSE}
library(reshape2)
library(ggplot2)
library(dplyr)
library(psych)
library(MASS)
library(car)
library(corrplot)
library(caret)
library(performance) # overall model performace
library(AICcmodavg) # model selection
library(bestglm)
```

## Explore Data

### stats

The data set has 13 variables and 466 cases.

```{r}
# load dataset
data <- read.csv('crime-training-data_modified.csv')
str(data)
```

From the summary, it does not seem to have missing value. However, variables like 'zn', 'indus', 'age' and so on are skewed, additionally, variable 'chas' and response variable should be factor type instead of int

```{r}
# brief summary
summary(data)
```

### plots

from boxplot, predictors are split by response variable, except 'nox', 'rad', and dummy variable 'chas' have no outliers, the rest of them have more/less outliers

```{r}
# make box plot for all variables
data.m <- melt(data, id.vars = 'target')%>% mutate(target = as.factor(target))
head(data.m)
ggplot(data.m, aes(x = variable, y = value, fill = target)) + geom_boxplot() + facet_wrap(~ variable, scales = 'free') + theme_classic()
```

As I mentioned above, there are some variables skewed(not include dummy variable), some are bi-model, only 'rm' looks pretty normal.

```{r}
# check normality and skewness
data.m %>% ggplot(aes(x = value)) + geom_density() + facet_wrap(~variable, scale = 'free') + theme_classic()
```

From correlation plot, variables between have somewhat relationship. However, from correlation scatter plot, it is very hard to tell which variable has which kind of relationship with others because all of points seems cluster together.

```{r warning=FALSE, message=FALSE}
# correlation between predictors and response
par(mar = c(1,1,1,1))
data%>% corPlot()
```

```{r}
# shape of correlation
pairs(data)
```

## Data Transformation

### transform type

```{r}
data$chas <- as.factor(data$chas)
data$target <- as.factor(data$target)
```

### transform variable

corresponding library is required to perform the function, [reference is here](https://machinelearningmastery.com/pre-process-your-dataset-in-r/).

use Box-Cox transformation as well as center and scale, just so variables are in the same scale, from the plot, we see that variables are more normal than raw data. Even though there are some variables still look problematic, let's stay with it now.

```{r}
# use transform data using boxcox, in the mean time center and scale them.
preprocess <- preProcess(data %>% dplyr::select(-target), method = c('BoxCox', 'center', 'scale'))

# get transformed data set=
trans.df <- predict(preprocess, data %>% dplyr::select(-target)) %>% mutate(target = as.factor(data$target))

# plot density to check skewness and normality
trans.df.m <- trans.df %>% mutate(target = as.factor(data$target)) %>% melt(id.vars = 'target')

trans.df.m %>% ggplot(aes(x = value)) + geom_density() + facet_wrap(~variable, scale = 'free')
```

```{r warning=FALSE, message=FALSE, eval=FALSE, include=FALSE}
# for my personal reference
attach(data)
back_transform <- function(y, lamda){
  if(lamda ==0L){
    log(y)
  }else{
    y^lamda/lamda
  }
}
# transform nox
nox.bc <- boxcox(lm(nox ~ 1, data))
nox.pow <- nox.bc$x[which.max(nox.bc$y)]
nox.t <- back_transform(nox, nox.pow)

# transform age
age.bc <- boxcox(lm(age ~ 1, data))
age.pow <- age.bc$x[which.max(age.bc$y)]
age.t <- back_transform(age, age.pow)

# transform dis
dis.bc <- boxcox(lm(dis ~ 1, data))
dis.pow <- dis.bc$x[which.max(dis.bc$y)]
dis.t <- back_transform(dis, dis.pow)

# transform rad
rad.bc <- boxcox(lm(rad ~ 1, data))
rad.pow <- rad.bc$x[which.max(rad.bc$y)]
rad.t <- back_transform(rad, rad.pow)

# transform tax
tax.bc <- boxcox(lm(tax ~ 1, data))
tax.pow <- tax.bc$x[which.max(tax.bc$y)]
tax.t <- back_transform(tax, tax.pow)

# transform ptratio
ptratio.bc <- boxcox(lm(ptratio ~ 1, data))
ptratio.pow <- ptratio.bc$x[which.max(ptratio.bc$y)]
ptratio.t <- back_transform(ptratio, ptratio.pow)

# transform medv
medv.bc <- boxcox(lm(medv ~ 1, data))
medv.pow <- medv.bc$x[which.max(medv.bc$y)]
medv.t <- back_transform(medv, medv.pow)

subset <- data.frame(target, nox.t, age.t, dis.t, rad.t, tax.t, ptratio.t, medv.t)
```

## building model

### split data into train set and test set

```{r}
set.seed(100)

split.df <- trainTestPartition(trans.df, 0.7) # 70% is training, the rest will be testing

train <- split.df$XyTr  # training set with label
test <- split.df$XyTe  # testing test with label

nrow(train)
nrow(test)
```

### logistic regression

#### first model(full)

-   the distribution of deviance residuals looks pretty normal, there are more than one predictors are not statistically significant

-   AIC is 171.47

```{r}
m1 <- glm(target ~ . , data = train, family = binomial)
summary(m1)
```

-   the plot prove that predictors like 'zn', 'indus' and so on with slope near 0 are not so useful for predicting result

```{r}
# added variable plot
par(mar = c(0.3, 0.3, 0.3, 0.3))
avPlots(m1)
```

-   from the model, we see that variable 'medv' and 'rm' happen to have collinearity

-   from marginal plot, line for data and line for model is indistinguishable, it happens to be a valid model.

```{r}
# colinearity checking
vif(m1)

# validity check
mmps(m1)
```

### second model(reduced - remove medv, rm)

To solve collinearity, I am going to remove variables cause such problem to form a new model.

-   the distribution of deviance residuals still looks fine

-   AIC increase to 176.22

```{r}
m2 <- update(m1, .~. -medv -rm)
summary(m2)

```

-   there is no collinearity

-   model is valid

```{r}
# colinearity
vif(m2)

# mmps
mmps(m2)
```

### third model (stepwise based on m1)

stepwise approach helps to find all subset of proper models and pick the lowest AIC score(which means the best model it computes). Therefore, there is no need for me to do backward elimination and forward selection.

-   the final model it picks has AIC score of 162.05

```{r}
m3 <- step(update(m1, .~.))
summary(m3)
```

-   there is no colinearity and model is valid

```{r}
# collinearity
vif(m3)

# validity
mmps(m3)
```

#### Forth model(stepwise based on m2)

m1 has the full model, m2 is reduced model.

-   the best model computed by step() function has AIC score of 171.71 which is very similar to m1

```{r}
m4 <- step(update(m2, .~.))
summary(m4)
```

-   no colinearity and model is still valid

```{r}
# colinearity check
vif(m4)

# validity check
mmps(m4)
```

## model selection

based on the result from AICc, and model performance, I am going to choose model3

```{r}
model_list <- list(m1, m2, m3, m4)
aictab(model_list)
```

```{r}
compare_performance(m1, m2, m3, m4, rank = T)
```

## prediction

```{r}

pred <- predict(m3, test)
pred[pred>0.5] = 1
pred[pred<0.5] = 0
pred <- pred %>% as.factor()

confusionMatrix(pred, test$target)
```